\chapter{绪论}


\section{选题背景及意义}
复杂边界流动现象广泛存在于石油天然气开采、化工过程、新能源技术、医学、 环境保护等领域。
这类现象可以抽象为流体在多孔介质中的流动。由于流固边界高度复杂，通常只能用实验方法和数值方法研究这类现象。
目前用实验方法研究真实多空介质流动时，还缺乏有效的实验设备观测细微孔隙结构中流动现象，并且实验方法
还具有费用昂贵、实验周期长、参数控制不易、流场数据获取不够全面等缺点。伴随着计算机技术的发展，
研究这类现象的计算流体动力学方法发展越来越成熟，并且没有实验研究的诸多限制和缺点，已经成为一个重要的研究手段。
其中格子Boltzmann方法（LBM）作为一种介观方法，相对于传统CFD方法，无须显式进行空间网格划分，处理复杂的流固边界简单，
因此特别适合于在孔隙尺度上模拟多孔介质流动。另外，由于它在微观上基于分子动力学，
继承了分子动力学刻画分子间相互作用简单的特点，而宏观现象如组分间和相间相互作用的微观本质就是分子间相互作用，
因此用格子Boltzmann方法模拟多组分和多相流动也具有很大优势， 比如它能自动捕捉复杂的相界面\ucite{He2002}。
但由于LBM是一种显式的时间推进方法，并且空间离散点较多，所以计算量通常非常大,
基于LBM的实际研究或工程应用一般都要利用并行计算加速\ucite{pohl2004performance, geller2006benchmark}。
标准的LBM算法中粒子碰撞迁移都具有局部性，因此进行大规模并行计算通常都能获得良好的并行效率和可扩展性。
早期的LBM并行程序大多数是基于消息传递函数接口协议（MPI），运行在通过高速网络互连的商业计算机集群上面。
而最近几年刚刚兴起的通用计算图形处理器（GPGPU）则为LBM并行计算提供一条更高效、更廉价、更节能的途径。

图形处理器最开始作为CPU的协处理器，用于加速图形、图像处理等计算密集性任务。
早期的GPU被用作固定函数的流水线，而经过多年的发展，GPU的可变编程性逐渐增强，
这使得将其用于非图形学领域的通用科学计算成为可能。
早在2000年，就有计算机科学家及电磁学研究领域的研究者用GPU来加速他们的科学计算程序，
这就是后来兴起的GPGPU的开端。
GPU相对于CPU提供的处理能力和存储带宽要大得多，
目前市场上主流的GPU浮点数处理能力是同期CPU的10倍左右， 而存储带宽则是CPU的5倍左右\ucite{cudaguide}。
并且受到市场需求的推动，GPU的这一优势还在扩大。在价格和能耗上GPU相对于CPU也具有较大优势。
%在提供相同计算能力或存储带宽的情况下，利用配备了GPU的服务器或工作站甚至普通PC机，
如一台配置了较新型号消费级GPU的普通PC机，其单精度峰值计算能力可以达到Tflops级别\ucite{tolke2008teraflop}。

GPU生产商NVIDIA公司于2007年推出了CUDA(统一计算构架), 利用CUDA技术进行GPGPU的编程难度
大为降低。在CUDA推出之前，要想将GPU用于非图形学的科学计算，则必须将这种计算以图形学的语言来描述，
如将普通的计算（数据操作）映射为光栅化和帧缓冲等图形操作，将普通数据映射为纹理等图形数据。
这种面向图形应用领域硬件层面的编程方式，对于非图形学领域的工作者而言，入门门槛较高。
CUDA技术使用的是一种经过扩展的C语言，它在标准的C语言中加入了能调用GPU计算功能的并行机制，
并提供了包含编译器驱动、常用并行函数库等在内的一整套完整的开发环境。
熟悉C语言的用户可以用一种很自然的方式来编制GPU并行程序。
CUDA推出之后，并很快在科学计算领域流行起来，目前有越来越多的国内外研究单位
开始利用该技术加速计算，国内外一些大学亦开始开设了GPGPU高性能计算的课程。
NVIDIA公司同时也推出了专用于加速科学计算应用的高端GPU系列产品\---Tesla GPU。
在2012年11月公布的全球超级计算机性能排行榜TOP500中，
位列第一名的Titan超级计算机就使用了261632个Kepler构架的K20GPU作为加速器，
其理论峰值计算能力达到了27PFlops（$2.7\times10^7$GFlops）。
目前已经有分子动力学、计算流体力学、计算结构力学、计算化学、医学成像、
计算金融等一系列科学计算应用被被移植到GPU\ucite{gpuapp}，并获得了相对于CPU数倍甚至一个量级以上的的性能提升。


相对于其它的计算流体动力学方法，LBM在GPU上往往能获得更高的加速比，甚至在单块GPU上就能获得相对于CPU两个
量级的加速比。究其原因，一是因为在LBM计算过程中的访存具有空间局部性，每个节点的碰撞、
迁移过程只涉及到存取临近格点的数据。 能充分发挥GPU高带宽的优势；
二是因为在 LBM计算与图形、图像处理的计算、访存模式十分相近\---
对大量同类数据元素进行某种简单的操作，这种模式非常适合于细粒度的并行化处理。

利用GPU加速，能大幅缩短LBM模拟的计算时间，在三维高分辨率模拟时，这一优势尤为明显。
如原本需要一天时间运行的串行程序，利用GPU加速，在十几分钟内并可以算完。
在孔隙尺度进行LBM模拟时，通常分辨率要求非常高，用常规CPU串行程序模拟时特别耗时。
本文的目的就是利用GPU实现三维多空介质中单相和多相流动LBM模拟，
并研究相应的性能优化方法。

\section{本课题国内外研究情况}
早在2003年， Li等人最先在GPGPU上实现了LBM并行计算\ucite{liu2004real}，其做法是将速度分布函数映射为二维纹理数据，
并将格子Boltzmann方程完全映射为光栅化和帧缓冲操作。他们按这种方式实现的GPU程序相比CPU版本的程序，
计算速度提速了十多倍。
随后又更多的人在GPU上实现LBM并行计算。如文献\cite{chu2005moxi}中,作者模拟了墨水在吸水性纸张上扩散的过程。
Fan等人搭建了有30个GPU节点的计算机集群，并用LBM在上面模拟了纽约时代广场上的空气污染物的扩散过程\ucite{fan2004gpu}。
文献\cite{zhu2006simulation}和\cite{wei2004lattice}分别模拟了多相环境的熔化和肥皂泡及羽毛的运动。
值得指出的是，这些工作都是在CUDA技术推出之前进行的，所以都是用计算机图形学的编程方法实现的。

自NVIDIA公司于2007年推出CUDA技术后，Jonas T\"olke利用该技术在NVIDIA 8800Ultra 显卡上实现了二维的LBM并行计算，
并获得了相对于标准CPU一个量级以上的性能提升 \ucite{tolke2010implementation} 。
在该文中，作者首次提出利用GPU的共享内存辅助完成LB计算过程中的迁移步， 以达到合并访问进而最大限度利用GPU提供的带宽。
Jonas T\"olke随后又将该方法用于三维LBM模拟，结合一种特殊的D3Q13格子模型，该方法在单个GPU上达到了两个量级的加速比\ucite{tolke2008teraflop}。

由于单块GPU上搭载的显存大小有限，不能满足大规模模拟需求，而如果将MPI的分布式存储技术与GPU结合，
在通过高速网络互连的各个计算节点上搭载GPU，构造多GPU集群，则可以有效解决这个问题。
在这种构架中，GPU 和 GPU之间的通信以CPU作为中间层，利用当今流行的MPI标准实现跨节点数据传输。
上一节提到的Titan超级计算机利用的就是这种构架。在LBM的多GPU并行化方面，
Obrecht等人\ucite{obrecht2011multi, obrecht2011thelma}以及国内中科院过程所李博 \ucite{xiong2012efficient, xiong2012large}
等人做了不少相关工作。关于LBM的GPU并行程序性能分析及优化方面的工作可见文献
\cite{habich2008performance, huang2011, ren2010optimization, myre2011performance}。

文献\cite{aksnesporous, aksnesporous, tolke2010computer}中专门研究了多孔介质孔隙尺度LBM模拟的GPU并行化。
考虑更复杂的流动如多相、多组分LBM模拟的GPU并行实现，目前文献中并不多见。
Myre等人\ucite{myre2011performance}运用控制变量法详细测试并分析了单组分、多组分LBM的多GPU程序的性能，
并得出了影响GPU程序性能的各个因素的相对重要性。 Rosales等人\ucite{rosales2011multiphase}用多GPU实现的多相LBM程序
相对于单核CPU提速了40倍。
Redapangu等人\ucite{redapangu2012multiphase}利用GPU实现的多相LB模型研究了浮力驱动的非混溶液体流动，相对CPU加速了25倍。
这些文献中的流场多为简单流场。通过文献调研我们发现，针对多孔介质多相渗流的GPU实现，目前只有T\"olke等人进行了相关研究\ucite{2011AGUFMIN, 2009AGUFM}。
他们使用的多相模型是颜色LBE模型，这是一种早期的多相流模型\ucite{gunstensen1991lattice}，较后来提出的的Shan-Chen模型及其改进模型
存在一些缺点\ucite{guoredbook}。

\section{本文的主要研究内容和组织结构}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% 我的工作。。。。。
\subsection{研究内容}
本论文主要的研究内容包括在GPU平台利用CUDA技术实现单组分单相渗流、
多组分多相渗流格子Boltzmann数值模拟的并行计算，开发基于GPU\footnote{目前是单GPU环境}的
多相渗流高性能计算程序。具体研究内容包括如下方面：
\begin{enumerate}
  \item 实现目前文献中常见的针对简单流动问题的LBM模拟GPU程序；
  \item 分析多空介质孔隙尺度单组份单相流动LB模拟的GPU程序性能优化方法；
  \item 高效实现多空介质孔隙尺度多组分多相流动LB模拟的GPU程序，
    使用的多相模型为一种基于Shan-Chen模型的改进模型。
\end{enumerate} 

\subsection{章节安排}
本文的组织结构如下：

第一章是绪论，介绍本文研究课题的相关背景、意义以及国内外关于本课题的研究情况。

第二章介绍格子Boltzmann方法的基本原理、边界处理及本文使用的多相LBE作用力模型。

第三章介绍现代GPU构架和CUDA技术以及GPU程序设计和优化的基本原则。

第四章介绍了目前文献上常见的LBM在GPU上实现方法，并利用我们实现的GPU程序模拟了
二维方腔流和外力驱动的三维方截面直管道流动问题，验证了程序正确性并进行了程序
性能测试。

第五章介绍了进行多孔介质孔隙尺度模拟时的减少计算量和存储空间的稀疏存储算法，
并提出了一种结合位操作和逻辑运算的指令优化技术。随后分别针对这两种优化方法
编制了三维GPU程序，模拟了BCC多孔介质结构中的单组分单相渗流，
测试了其绝对渗透率验证了，并详细分析了影响程序性能的各种因素。

第六章针对多组分LB模拟的特点对第5章提出的算法进行了重新设计，并开发了相应的
的CUDA程序。先用两个标准算例验证了程序的正确性，随后模拟了真实多孔介质中的多相渗流。
最后分析了程序的性能。

第七章总结全文并展望未来的研究工作。
